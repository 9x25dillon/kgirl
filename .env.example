# =============================================================================
# kgirl - Multi-Framework LLM Knowledge Platform
# Environment Configuration
# =============================================================================

# -----------------------------------------------------------------------------
# ðŸš€ LOCAL-FIRST CONFIGURATION (No API Keys Needed!)
# -----------------------------------------------------------------------------
# By default, kgirl uses Ollama for local LLM inference - completely free!
# Uncomment and configure the sections below only if you want to use cloud APIs

# Use local LLMs via Ollama (true = local, false = cloud APIs)
USE_LOCAL_LLM=true

# -----------------------------------------------------------------------------
# Ollama Local LLM Configuration
# -----------------------------------------------------------------------------
# Ollama chat model (free, runs locally)
OLLAMA_CHAT_MODEL=qwen2.5:3b

# Ollama embedding model (free, runs locally)
OLLAMA_EMBED_MODEL=nomic-embed-text

# Ollama service host
OLLAMA_HOST=http://localhost:11434

# -----------------------------------------------------------------------------
# Multi-Model Pool Configuration
# -----------------------------------------------------------------------------
# Define available models in format: "provider:param=value|provider:..."
#
# LOCAL ONLY (default - no API keys needed):
MODELS=ollama:chat=qwen2.5:3b,embed=nomic-embed-text
#
# CLOUD ONLY (requires API keys below):
# MODELS=openai:chat=gpt-4o-mini,embed=text-embedding-3-large|anthropic:chat=claude-3-5-sonnet-latest
#
# HYBRID (local + cloud consensus):
# MODELS=ollama:chat=qwen2.5:3b,embed=nomic-embed-text|openai:chat=gpt-4o-mini,embed=text-embedding-3-large

# -----------------------------------------------------------------------------
# Cloud API Keys (OPTIONAL - only if using cloud providers)
# -----------------------------------------------------------------------------
# OpenAI API Key for GPT models and embeddings
# OPENAI_API_KEY=sk-...

# Anthropic API Key for Claude models
# ANTHROPIC_API_KEY=sk-ant-...

# -----------------------------------------------------------------------------
# Cloud LLM Model Configuration (OPTIONAL)
# -----------------------------------------------------------------------------
# OpenAI chat model (used for generation)
# OPENAI_CHAT_MODEL=gpt-4o-mini

# OpenAI embedding model (used for vector representations)
# OPENAI_EMBED_MODEL=text-embedding-3-large

# Anthropic chat model (used for generation)
# ANTHROPIC_CHAT_MODEL=claude-3-5-sonnet-latest

# -----------------------------------------------------------------------------
# Topological Consensus Parameters
# -----------------------------------------------------------------------------
# Central charge for topological field theory calculations (default: 627)
CENTRAL_CHARGE=627

# Number of anyons for phase coherence calculations (default: 5)
N_ANYONS=5

# -----------------------------------------------------------------------------
# Database Configuration
# -----------------------------------------------------------------------------
# PostgreSQL connection URL for ChaosRAG Julia service
# Format: postgres://username:password@host:port/database
DATABASE_URL=postgres://user:pass@localhost:5432/chaos

# -----------------------------------------------------------------------------
# Service Ports
# -----------------------------------------------------------------------------
# FastAPI Topological Consensus API port (main.py)
MAIN_API_PORT=8000

# ChaosRAG Julia service port (server.jl)
CHAOS_RAG_PORT=8001

# Ollama LLM service port
OLLAMA_PORT=11434

# LIMPS Julia service port (if using external LIMPS framework)
LIMPS_PORT=8000

# -----------------------------------------------------------------------------
# Optional: Topological Consciousness Library (CTH)
# -----------------------------------------------------------------------------
# Path to CTH library for advanced topological calculations
# Leave empty or comment out if not using CTH
# CTH_PATH=/path/to/cth

# -----------------------------------------------------------------------------
# Optional: Advanced Configuration
# -----------------------------------------------------------------------------
# Sentence transformers model for local embeddings (fallback)
# ST_MODEL=sentence-transformers/all-MiniLM-L6-v2

# Python environment mode (development/production)
# ENV=development

# Log level (DEBUG/INFO/WARNING/ERROR)
# LOG_LEVEL=INFO
